{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "W1BgTx9jTOQq"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "Fljr7CL_nHpD"
   },
   "outputs": [],
   "source": [
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "        \n",
    "class RetinaMultiLabelDataset(Dataset):\n",
    "    def __init__(self, csv_file, image_dir, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        img_path = os.path.join(self.image_dir, row.iloc[0])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        labels = torch.tensor(row[1:].values.astype(\"float32\"))\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, labels\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=[0.5, 0.85, 0.85], gamma=2.0): #Best:0.5 0.9 0.9\n",
    "        super().__init__()\n",
    "        if alpha is not None:\n",
    "            self.alpha = torch.tensor(alpha, dtype=torch.float)\n",
    "        else:\n",
    "            self.alpha = None\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        p = torch.sigmoid(inputs)\n",
    "        p_t = p * targets + (1 - p) * (1 - targets)\n",
    "        focal_weight = (1 - p_t) ** self.gamma\n",
    "        loss = focal_weight * bce_loss\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            alpha_t = self.alpha.to(inputs.device) * targets + (1 - targets)\n",
    "            loss = alpha_t * loss\n",
    "\n",
    "        return loss.mean()\n",
    "\n",
    "class ClassBalancedBCE(nn.Module):\n",
    "    def __init__(self, samples_per_class, beta=0.999, reduction='mean'):\n",
    "        super().__init__()\n",
    "        effective_num = 1.0 - beta ** samples_per_class\n",
    "        weights = (1.0 - beta) / effective_num.clamp(min=1e-6)\n",
    "        weights = weights / weights.sum() * len(samples_per_class)\n",
    "        self.register_buffer('weights', weights)\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        bce = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        weight = self.weights[targets.long().argmax(dim=1)]  # pick weight of positive class\n",
    "        weight = targets * self.weights[0] + (1 - targets) * self.weights[1]  # if you want per-class\n",
    "        loss = weight * bce\n",
    "        return loss.mean() if self.reduction == 'mean' else loss.sum()\n",
    "\n",
    "\n",
    "class ClassBalancedLoss(nn.Module):\n",
    "    def __init__(self, num_samples_per_class, beta=0.9):\n",
    "   \n",
    "        super().__init__()\n",
    "        num_samples_per_class = torch.tensor(num_samples_per_class, dtype=torch.float)\n",
    "        effective_num = 1.0 - beta ** num_samples_per_class\n",
    "        weights = (1.0 - beta) / torch.clamp(effective_num, min=1e-6)\n",
    "        weights = weights / weights.sum() * len(num_samples_per_class)\n",
    "        self.register_buffer('weights', weights)\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        weights = self.weights.to(inputs.device)  \n",
    "        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        modulator = targets * weights\n",
    "        loss = modulator * bce_loss\n",
    "        return loss.mean()\n",
    "\n",
    "class FocalCBLoss(nn.Module):\n",
    "    def __init__(self, num_samples_per_class, beta=0.5, gamma=2.0):\n",
    "        super().__init__()\n",
    "        # CB weights\n",
    "        num_samples = torch.tensor(num_samples_per_class, dtype=torch.float)\n",
    "        effective_num = 1.0 - beta ** num_samples\n",
    "        weights = (1.0 - beta) / torch.clamp(effective_num, min=1e-6)\n",
    "        weights = weights / weights.sum() * len(num_samples_per_class)\n",
    "        self.register_buffer('weights', weights)\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        weights = self.weights.to(inputs.device)\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        focal_loss = (1 - pt) ** self.gamma * bce_loss  \n",
    "        modulator = targets * weights  \n",
    "        loss = modulator * focal_loss\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "Ucz_QbPVnLJ4"
   },
   "outputs": [],
   "source": [
    "def build_model(backbone=\"efficientnet\", num_classes=3, pretrained=True, attention_type=None):\n",
    "    model = None\n",
    "\n",
    "    if backbone == \"resnet18\":\n",
    "        model = models.resnet18(pretrained=pretrained)\n",
    "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "    elif backbone == \"efficientnet\":\n",
    "        model = models.efficientnet_b0(pretrained=pretrained)\n",
    "\n",
    "        if attention_type == 'se':\n",
    "            channels = [16, 24, 40, 80, 112, 192, 320]\n",
    "            reduction = 16  \n",
    "\n",
    "            se_blocks = nn.ModuleList([SEBlock(ch, reduction=reduction) for ch in channels])\n",
    "\n",
    "            new_features = []\n",
    "            se_idx = 0\n",
    "            for i, module in enumerate(model.features):\n",
    "                if i in [1, 2, 3, 4, 5, 6, 7]:  \n",
    "                    if se_idx < len(se_blocks):\n",
    "                        wrapped = nn.Sequential(module, se_blocks[se_idx])\n",
    "                        new_features.append(wrapped)\n",
    "                        se_idx += 1\n",
    "                    else:\n",
    "                        new_features.append(module)\n",
    "                else:\n",
    "                    new_features.append(module)\n",
    "\n",
    "            model.features = nn.Sequential(*new_features)\n",
    "\n",
    "            model.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "            model.classifier = nn.Sequential(\n",
    "                nn.Flatten(),\n",
    "                nn.Dropout(p=0.2),\n",
    "                nn.Linear(1280, num_classes)\n",
    "            )\n",
    "\n",
    "        elif attention_type == 'mha':\n",
    "            embed_dim = 1280\n",
    "            num_heads = 4\n",
    "\n",
    "            class GlobalMHABlock(nn.Module):\n",
    "                def __init__(self, embed_dim, num_heads):\n",
    "                    super().__init__()\n",
    "                    self.mha = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "                    self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "                def forward(self, x):\n",
    "                    B, C, H, W = x.shape\n",
    "                    x_tokens = x.flatten(2).transpose(1, 2)\n",
    "                    attn_out, _ = self.mha(x_tokens, x_tokens, x_tokens)\n",
    "                    attn_out = self.norm(attn_out)\n",
    "                    return attn_out.mean(dim=1)\n",
    "\n",
    "            model.global_pool = nn.Identity()\n",
    "            model.mha_block = GlobalMHABlock(embed_dim, num_heads)\n",
    "            model.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "            def forward(x):\n",
    "                x = model.features(x)\n",
    "                x = model.mha_block(x)\n",
    "                x = model.head(x)\n",
    "                return x\n",
    "\n",
    "            model.forward = forward\n",
    "\n",
    "        else:\n",
    "            # Default\n",
    "            model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "\n",
    "    elif backbone == \"swin_tiny\":\n",
    "        import timm\n",
    "        model = timm.create_model('swin_tiny_patch4_window7_224', pretrained=pretrained, num_classes=num_classes)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"Unsupported backbone\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def set_finetune_mode(model, mode, backbone):\n",
    "    \"\"\"\n",
    "    mode:\n",
    "    1 = no finetuning\n",
    "    2 = frozen backbone, train classifier\n",
    "    3 = full finetuning\n",
    "    \"\"\"\n",
    "    if mode == 1:\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    elif mode == 2:\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = False\n",
    "        if backbone == \"resnet18\":\n",
    "            for p in model.fc.parameters():\n",
    "                p.requires_grad = True\n",
    "        elif backbone == \"efficientnet\":\n",
    "            for p in model.classifier.parameters():\n",
    "                p.requires_grad = True\n",
    "\n",
    "    elif mode == 3:\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "def finetune_mode_name(mode):\n",
    "    return {\n",
    "        1: \"no_ft\",\n",
    "        2: \"frozen\",\n",
    "        3: \"full\"\n",
    "    }[mode]\n",
    "\n",
    "\n",
    "\n",
    "def find_best_thresholds(y_true, y_prob, num_classes=3):\n",
    "    \"\"\"\n",
    "    Grid search for per-class thresholds that maximize average F1 on offsite test set.\n",
    "    y_true: numpy array (N, 3) binary labels\n",
    "    y_prob: numpy array (N, 3) probabilities from model\n",
    "    Returns: list of 3 best thresholds [thresh_DR, thresh_G, thresh_A]\n",
    "    \"\"\"\n",
    "    best_thresh = [0.5] * num_classes\n",
    "    best_avg_f1 = 0.0\n",
    "\n",
    "    thresholds = np.arange(0.2, 0.8, 0.05)  \n",
    "\n",
    "    for thresh_dr in thresholds:\n",
    "        for thresh_g in thresholds:\n",
    "            for thresh_a in thresholds:\n",
    "                thresh = np.array([thresh_dr, thresh_g, thresh_a])\n",
    "                preds = (y_prob > thresh).astype(int)\n",
    "\n",
    "                f1_dr = f1_score(y_true[:, 0], preds[:, 0])\n",
    "                f1_g = f1_score(y_true[:, 1], preds[:, 1])\n",
    "                f1_a = f1_score(y_true[:, 2], preds[:, 2])\n",
    "                avg_f1 = (f1_dr + f1_g + f1_a) / 3\n",
    "\n",
    "                if avg_f1 > best_avg_f1:\n",
    "                    best_avg_f1 = avg_f1\n",
    "                    best_thresh = [thresh_dr, thresh_g, thresh_a]\n",
    "\n",
    "    print(f\"Best thresholds: DR={best_thresh[0]:.3f}, G={best_thresh[1]:.3f}, A={best_thresh[2]:.3f}\")\n",
    "    print(f\"Best average F1 on offsite: {best_avg_f1:.4f}\")\n",
    "    return best_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "KkIEHljvncpG"
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_one_backbone(backbone, finetune_mode, train_csv, val_csv, test_csv, train_image_dir, val_image_dir, test_image_dir,\n",
    "                       epochs=50, batch_size=32, lr=1e-4, img_size=256, save_dir=\"/content/drive/MyDrive/Deep Learning P2 Oulu/final-project-deep-learning-fall-2025/final_project_resources/results/checkpoints\", pretrained_backbone=None,\n",
    "                       patience=5,focal_gamma=None, samples_per_class=None,loss_type='bce',attention_type=None):  # New: patience for early stopping\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # transforms\n",
    "    train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "    val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "    train_ds = RetinaMultiLabelDataset(train_csv, train_image_dir, train_transform)\n",
    "    val_ds   = RetinaMultiLabelDataset(val_csv, val_image_dir, val_test_transform)\n",
    "    test_ds  = RetinaMultiLabelDataset(test_csv, test_image_dir, val_test_transform)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    model = build_model(backbone, num_classes=3, pretrained=True, attention_type=attention_type).to(device)\n",
    "  \n",
    "    best_val_loss = float(\"inf\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    mode_name = finetune_mode_name(finetune_mode)\n",
    "    ckpt_path = os.path.join(save_dir, f\"{backbone}_{mode_name}.pt\")\n",
    "\n",
    "    # load pretrained backbone\n",
    "    if pretrained_backbone is not None:\n",
    "        state_dict = torch.load(pretrained_backbone, map_location=\"cpu\")\n",
    "        model.load_state_dict(state_dict)\n",
    "\n",
    "    set_finetune_mode(model, finetune_mode, backbone)\n",
    "\n",
    "    if finetune_mode == 1:\n",
    "        torch.save(model.state_dict(), ckpt_path)\n",
    "        print(f\"Saved pretrained model for {backbone} (no fine-tuning) at {ckpt_path}\")\n",
    "        return  \n",
    "\n",
    "\n",
    "    if loss_type == \"bce\":\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "    elif loss_type == \"focal\":\n",
    "        criterion = FocalLoss(alpha=1, gamma=focal_gamma)\n",
    "    elif loss_type == \"cb\":\n",
    "        criterion = ClassBalancedLoss(samples_per_class)\n",
    "    elif loss_type == 'focal_cb':\n",
    "        criterion = FocalCBLoss(samples_per_class, beta=0.9, gamma=focal_gamma)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown loss_type\")\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "    epochs_no_improve = 0\n",
    "    early_stop = False\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in val_loader:\n",
    "                imgs, labels = imgs.to(device), labels.to(device)\n",
    "                outputs = model(imgs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * imgs.size(0)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "\n",
    "\n",
    "        print(f\"[{backbone}] Epoch {epoch+1}/{epochs} Train Loss: {train_loss:.8f} Val Loss: {val_loss:.8f}\")\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), ckpt_path)\n",
    "            print(f\"Saved best model for {backbone} at {ckpt_path}\")\n",
    "            epochs_no_improve = 0  \n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"No improvement in val loss for {epochs_no_improve} epochs\")\n",
    "\n",
    "    \n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "            early_stop = True\n",
    "            break\n",
    "\n",
    "\n",
    "def evaluate_model(\n",
    "    ckpt_path,\n",
    "    backbone,\n",
    "    test_csv,\n",
    "    test_image_dir,\n",
    "    batch_size=32,\n",
    "    img_size=224,\n",
    "    attention_type=None  \n",
    "):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    test_ds = RetinaMultiLabelDataset(test_csv, test_image_dir, transform)\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "  \n",
    "    model = build_model(backbone, num_classes=3, pretrained=False, attention_type=attention_type)\n",
    "    model.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    y_true_list = []\n",
    "    y_prob_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in test_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            outputs = model(imgs)\n",
    "            probs = torch.sigmoid(outputs).cpu().numpy()\n",
    "            y_prob_list.append(probs)\n",
    "            y_true_list.append(labels.numpy())\n",
    "\n",
    "    y_true = np.concatenate(y_true_list)\n",
    "    y_prob = np.concatenate(y_prob_list)\n",
    "\n",
    "    disease_names = [\"DR\", \"Glaucoma\", \"AMD\"]\n",
    "    results = {}\n",
    "    preds = (y_prob > 0.5).astype(int)\n",
    "    for i, disease in enumerate(disease_names):\n",
    "        y_t = y_true[:, i]\n",
    "        y_p = preds[:, i]\n",
    "        results[disease] = {\n",
    "            \"precision\": precision_score(y_t, y_p, zero_division=0),\n",
    "            \"recall\": recall_score(y_t, y_p, zero_division=0),\n",
    "            \"f1\": f1_score(y_t, y_p, zero_division=0),\n",
    "        }\n",
    "    results[\"average_f1\"] = np.mean([results[d][\"f1\"] for d in disease_names])\n",
    "\n",
    "    return results\n",
    "\n",
    "class OnsiteDataset(Dataset):\n",
    "    def __init__(self, csv_file, image_dir, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        img_name = row['id']\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, img_name\n",
    "\n",
    "# Now define predict_onsite\n",
    "def predict_onsite(\n",
    "    ckpt_path,\n",
    "    backbone,\n",
    "    onsite_csv,\n",
    "    onsite_image_dir,\n",
    "    output_csv='submission.csv',\n",
    "    batch_size=32,\n",
    "    img_size=256,\n",
    "    best_thresholds=[0.50, 0.5, 0.55],\n",
    "    use_tta=True,\n",
    "    attention_type=None  # \n",
    "):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    base_transform = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    tta_transforms = [base_transform]\n",
    "    if use_tta:\n",
    "        tta_transforms.append(\n",
    "            transforms.Compose([\n",
    "                transforms.Resize((img_size, img_size)),\n",
    "                transforms.RandomHorizontalFlip(p=1.0),  \n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "        )\n",
    "\n",
    "    model = build_model(\n",
    "        backbone=backbone,\n",
    "        num_classes=3,\n",
    "        pretrained=False, \n",
    "        attention_type=attention_type\n",
    "    )\n",
    "    model.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_probs = []\n",
    "    image_ids = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for transform in tta_transforms:\n",
    "            dataset = OnsiteDataset(onsite_csv, onsite_image_dir, transform=transform)\n",
    "            loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "            probs_batch = []\n",
    "            ids_batch = []\n",
    "\n",
    "            for imgs, batch_ids in loader:\n",
    "                imgs = imgs.to(device)\n",
    "                outputs = model(imgs)\n",
    "                prob = torch.sigmoid(outputs).cpu().numpy()\n",
    "                probs_batch.append(prob)\n",
    "                ids_batch.extend(batch_ids)\n",
    "\n",
    "            all_probs.append(np.concatenate(probs_batch, axis=0))\n",
    "            if image_ids is None:\n",
    "                image_ids = ids_batch\n",
    "\n",
    "    final_probs = np.mean(all_probs, axis=0)\n",
    "\n",
    "    if best_thresholds is None:\n",
    "        best_thresholds = [0.50, 0.5, 0.55]\n",
    "    preds = (final_probs > np.array(best_thresholds)).astype(int)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'id': image_ids,\n",
    "        'D': preds[:, 0],\n",
    "        'G': preds[:, 1],\n",
    "        'A': preds[:, 2]\n",
    "    })\n",
    "\n",
    "    df = df.sort_values('id').reset_index(drop=True)\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Saved onsite predictions to {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HqzMlc2Rnjk_",
    "outputId": "9a0846a6-d4d6-40fa-94ea-d6882101810e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[swin_tiny] Epoch 1/100 Train Loss: 0.20187182 Val Loss: 0.22807651\n",
      "Saved best model for swin_tiny at final_project_resources/results/checkpoints_task4\\swin_tiny_full.pt\n",
      "[swin_tiny] Epoch 2/100 Train Loss: 0.19273028 Val Loss: 0.20975644\n",
      "Saved best model for swin_tiny at final_project_resources/results/checkpoints_task4\\swin_tiny_full.pt\n",
      "[swin_tiny] Epoch 3/100 Train Loss: 0.16557607 Val Loss: 0.19626350\n",
      "Saved best model for swin_tiny at final_project_resources/results/checkpoints_task4\\swin_tiny_full.pt\n",
      "[swin_tiny] Epoch 4/100 Train Loss: 0.15297746 Val Loss: 0.18066468\n",
      "Saved best model for swin_tiny at final_project_resources/results/checkpoints_task4\\swin_tiny_full.pt\n",
      "[swin_tiny] Epoch 5/100 Train Loss: 0.14586738 Val Loss: 0.16349436\n",
      "Saved best model for swin_tiny at final_project_resources/results/checkpoints_task4\\swin_tiny_full.pt\n",
      "[swin_tiny] Epoch 6/100 Train Loss: 0.12949314 Val Loss: 0.14923015\n",
      "Saved best model for swin_tiny at final_project_resources/results/checkpoints_task4\\swin_tiny_full.pt\n",
      "[swin_tiny] Epoch 7/100 Train Loss: 0.11842428 Val Loss: 0.17420718\n",
      "No improvement in val loss for 1 epochs\n",
      "[swin_tiny] Epoch 8/100 Train Loss: 0.11217835 Val Loss: 0.15458626\n",
      "No improvement in val loss for 2 epochs\n",
      "[swin_tiny] Epoch 9/100 Train Loss: 0.11087898 Val Loss: 0.17307976\n",
      "No improvement in val loss for 3 epochs\n",
      "[swin_tiny] Epoch 10/100 Train Loss: 0.10563839 Val Loss: 0.14961687\n",
      "No improvement in val loss for 4 epochs\n",
      "[swin_tiny] Epoch 11/100 Train Loss: 0.09211251 Val Loss: 0.13605583\n",
      "Saved best model for swin_tiny at final_project_resources/results/checkpoints_task4\\swin_tiny_full.pt\n",
      "[swin_tiny] Epoch 12/100 Train Loss: 0.07258804 Val Loss: 0.15680966\n",
      "No improvement in val loss for 1 epochs\n",
      "[swin_tiny] Epoch 13/100 Train Loss: 0.08575487 Val Loss: 0.17822188\n",
      "No improvement in val loss for 2 epochs\n",
      "[swin_tiny] Epoch 14/100 Train Loss: 0.07220047 Val Loss: 0.19801038\n",
      "No improvement in val loss for 3 epochs\n",
      "[swin_tiny] Epoch 15/100 Train Loss: 0.06521350 Val Loss: 0.15379476\n",
      "No improvement in val loss for 4 epochs\n",
      "[swin_tiny] Epoch 16/100 Train Loss: 0.05733634 Val Loss: 0.16079320\n",
      "No improvement in val loss for 5 epochs\n",
      "Early stopping triggered after 16 epochs\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  samples_per_class = [517,163,142]\n",
    "\n",
    "  experiments = [\n",
    "    (\"resnet18\", 1),\n",
    "    (\"resnet18\", 2),\n",
    "    (\"resnet18\", 3),\n",
    "    (\"efficientnet\", 1),\n",
    "    (\"efficientnet\", 2),\n",
    "    (\"efficientnet\", 3),\n",
    "  ]\n",
    "  pretrained_paths = {\n",
    "        \"resnet18\": 'final_project_resources/pretrained_backbone/ckpt_resnet18_ep50.pt',  \n",
    "        \"efficientnet\": 'final_project_resources/pretrained_backbone/ckpt_efficientnet_ep50.pt'  \n",
    "  }\n",
    "  train_csv = \"final_project_resources/train.csv\" # replace with your own train label file path\n",
    "  val_csv   = \"final_project_resources/val.csv\" # replace with your own validation label file path\n",
    "  test_csv  = \"final_project_resources/offsite_test.csv\"  # replace with your own test label file path\n",
    "  train_image_dir =\"final_project_resources/images/train\"   # replace with your own train image floder path\n",
    "  val_image_dir = \"final_project_resources/images/val\"  # replace with your own validation image floder path\n",
    "  test_image_dir = \"final_project_resources/images/offsite_test\" # replace with your own test image floder path\n",
    "\n",
    "\n",
    "  '''for backbone, mode in experiments:\n",
    "      train_one_backbone(\n",
    "          backbone=backbone,\n",
    "          finetune_mode=mode,\n",
    "          train_csv=train_csv,\n",
    "          val_csv=val_csv,\n",
    "          test_csv=test_csv,  # Offsite for evaluation\n",
    "          train_image_dir=train_image_dir,\n",
    "          val_image_dir=val_image_dir,\n",
    "          test_image_dir=test_image_dir,\n",
    "          epochs=200,\n",
    "          lr=1e-3 if mode != 3 else 1e-4,\n",
    "          patience=10,\n",
    "          pretrained_backbone=pretrained_paths.get(backbone)  # None if not found, but you need them\n",
    "      )'''\n",
    "\n",
    "  train_one_backbone(\n",
    "    backbone=\"swin_tiny\",          \n",
    "    finetune_mode=3,\n",
    "    train_csv=train_csv,\n",
    "    val_csv=val_csv,\n",
    "    test_csv=test_csv,\n",
    "    train_image_dir=train_image_dir,\n",
    "    val_image_dir=val_image_dir,\n",
    "    test_image_dir=test_image_dir,\n",
    "    epochs=100,\n",
    "    batch_size=16,                 \n",
    "    lr=1e-4,                       \n",
    "    img_size=224,                  \n",
    "    save_dir=\"final_project_resources/results/checkpoints_task4\",\n",
    "    pretrained_backbone=None,       \n",
    "    patience=5,\n",
    "    loss_type='focal',              \n",
    "    focal_gamma=1.3,\n",
    "    attention_type=None             \n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "utfm9jyfNYO_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EVALUATING ON OFFSITE TEST SET ===\n",
      "\n",
      "Evaluating: Swin\n",
      "   DR: Precision=0.9044, Recall=0.8786, F1=0.8913\n",
      "   Glaucoma: Precision=0.8889, Recall=0.6531, F1=0.7529\n",
      "   AMD: Precision=0.6250, Recall=0.6818, F1=0.6522\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''experiments = [\n",
    "    {\n",
    "        \"name\": \"ResNet18 - No Fine-tuning\",\n",
    "        \"backbone\": \"resnet18\",\n",
    "        \"ckpt_path\": \"/content/drive/MyDrive/Deep Learning P2 Oulu/final-project-deep-learning-fall-2025/final_project_resources/results/checkpoints/resnet18_no_ft.pt\",\n",
    "        \"mode\": \"no_ft\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ResNet18 - Frozen Backbone\",\n",
    "        \"backbone\": \"resnet18\",\n",
    "        \"ckpt_path\": \"/content/drive/MyDrive/Deep Learning P2 Oulu/final-project-deep-learning-fall-2025/final_project_resources/results/checkpoints/resnet18_frozen.pt\",\n",
    "        \"mode\": \"frozen\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ResNet18 - Full Fine-tuning\",\n",
    "        \"backbone\": \"resnet18\",\n",
    "        \"ckpt_path\": \"/content/drive/MyDrive/Deep Learning P2 Oulu/final-project-deep-learning-fall-2025/final_project_resources/results/checkpoints/resnet18_full.pt\",\n",
    "        \"mode\": \"full\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"EfficientNet - No Fine-tuning\",\n",
    "        \"backbone\": \"efficientnet\",\n",
    "        \"ckpt_path\": \"/content/drive/MyDrive/Deep Learning P2 Oulu/final-project-deep-learning-fall-2025/final_project_resources/results/checkpoints/efficientnet_no_ft.pt\",\n",
    "        \"mode\": \"no_ft\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"EfficientNet - Frozen Backbone\",\n",
    "        \"backbone\": \"efficientnet\",\n",
    "        \"ckpt_path\": \"/content/drive/MyDrive/Deep Learning P2 Oulu/final-project-deep-learning-fall-2025/final_project_resources/results/checkpoints/efficientnet_frozen.pt\",\n",
    "        \"mode\": \"frozen\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"EfficientNet - Full Fine-tuning\",\n",
    "        \"backbone\": \"efficientnet\",\n",
    "        \"ckpt_path\": \"/content/drive/MyDrive/Deep Learning P2 Oulu/final-project-deep-learning-fall-2025/final_project_resources/results/checkpoints/efficientnet_full.pt\",\n",
    "        \"mode\": \"full\"\n",
    "    },\n",
    "]'''\n",
    "\n",
    "experiments = [\n",
    "\n",
    "    {\n",
    "        \"name\": \"Swin\",\n",
    "        \"backbone\": \"swin_tiny\",\n",
    "        \"ckpt_path\": \"final_project_resources/results/checkpoints_task4/swin_tiny_full.pt\",\n",
    "        \"mode\": \"full\"\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "offsite_test_csv = 'final_project_resources/offsite_test.csv'\n",
    "offsite_image_dir = 'final_project_resources/images/offsite_test'\n",
    "\n",
    "onsite_csv = 'final_project_resources/onsite_test_submission.csv'\n",
    "onsite_image_dir = 'final_project_resources/images/onsite_test'\n",
    "predictions_dir = 'final_project_resources/predictions'\n",
    "\n",
    "os.makedirs(predictions_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "all_offsite_results = {}\n",
    "\n",
    "print(\"=== EVALUATING ON OFFSITE TEST SET ===\\n\")\n",
    "for exp in experiments:\n",
    "    name = exp[\"name\"]\n",
    "    backbone = exp[\"backbone\"]\n",
    "    ckpt = exp[\"ckpt_path\"]\n",
    "\n",
    "    print(f\"Evaluating: {name}\")\n",
    "    results = evaluate_model(\n",
    "        ckpt_path=ckpt,\n",
    "        backbone=backbone,\n",
    "        test_csv=offsite_test_csv,\n",
    "        test_image_dir=offsite_image_dir, attention_type=None\n",
    "    )\n",
    "\n",
    "    all_offsite_results[name] = results\n",
    "\n",
    "    # Print nicely\n",
    "\n",
    "    for disease in [\"DR\", \"Glaucoma\", \"AMD\"]:\n",
    "        print(f\"   {disease}: Precision={results[disease]['precision']:.4f}, \"\n",
    "              f\"Recall={results[disease]['recall']:.4f}, \"\n",
    "              f\"F1={results[disease]['f1']:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "Rya2GxjcXIpR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GENERATING ONSITE PREDICTIONS FOR KAGGLE SUBMISSION ===\n",
      "\n",
      "Generating submission for: Swin → final_project_resources/predictions\\submission_Task2efficient_focal_81swin_tiny_full.csv\n",
      "Saved onsite predictions to submission_task4_swin_final4.csv\n",
      "Saved: final_project_resources/predictions\\submission_Task2efficient_focal_81swin_tiny_full.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== GENERATING ONSITE PREDICTIONS FOR KAGGLE SUBMISSION ===\\n\")\n",
    "for exp in experiments:\n",
    "    name = exp[\"name\"]\n",
    "    backbone = exp[\"backbone\"]\n",
    "    ckpt = exp[\"ckpt_path\"]\n",
    "    mode = exp[\"mode\"]\n",
    "\n",
    "    # Clean filename (replace spaces and special chars)\n",
    "    safe_name = name.replace(\" \", \"_\").replace(\"-\", \"_\").lower()\n",
    "    output_csv = os.path.join(predictions_dir, f\"submission_Task2efficient_focal_81{backbone}_{mode}.csv\")\n",
    "\n",
    "    print(f\"Generating submission for: {name} → {output_csv}\")\n",
    "\n",
    "    predict_onsite(\n",
    "        ckpt_path=\"final_project_resources/results/checkpoints_task4/swin_tiny_full.pt\",\n",
    "        backbone=\"swin_tiny\",\n",
    "        onsite_csv=onsite_csv,\n",
    "        onsite_image_dir=onsite_image_dir,\n",
    "        output_csv=\"submission_task4_swin_final4.csv\",\n",
    "        img_size=224,                   \n",
    "        use_tta=True,\n",
    "        best_thresholds=[0.5,0.5,0.5]  \n",
    "    )\n",
    "\n",
    "    print(f\"Saved: {output_csv}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "monzYuyN6Xkx"
   },
   "source": [
    "What more to try? Treshholds on onsite inference, Increase Lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V5E1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
